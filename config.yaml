admin : HYPE연어
train:
  seed : 42
model:
  model_name : klue/roberta-large
  batch_size : 16
  epoch : 3
  LR : 0.00001
  optim : AdamW
  scheduler : StepLR
  loss_function : CrossEntropyLoss #[FocalLoss, CrossEntropyLoss] 중 선택
  train_path : ./data/train_dataset
  test_path : ./data/test_dataset
  retrieval : bm25 # [sparse, bm25] 중 선택
  add_ce : True #bm25 이후 cross encoder를 사용한다면 True, 사용하지 않는다면 False
  bert : False #roberta를 사용한다면 false로 설정, bert를 사용한다면 true로 설정
  precision : 16 # 기본설정은 32s
EarlyStopping:
  turn_on: True # False
  monitor: val_em # [val_loss, val_f1, val_em] 중 선택
  min_delta: 0.00
  patience: 3
  verbose: True
data:
  overwrite_cache : False
  preprocessing_num_workers : 4
  max_seq_length : 512
  pad_to_max_length : True
  doc_stride : 128
  max_answer_length : 30
  eval_retrieval : True
  num_clusters : 64
  top_k_retrieval : 30
  use_faiss : False
  use_sub: False
  use_normalize: False
  use_drop_duplicated_wiki: True
  use_drop_less_than_50_percent_of_korean: True
  use_drop_too_long_text: True
  use_add_title_to_text: True

sweepcnt : 2 # sweep을 반복할 횟수(체크포인트 용량으로 인해 30이하 권장)
sweep:
  project : MRC
  entity : HYPE연어
  method: grid # random, grid, bayes
  name: sweep
  metric:
    name: val_em
    goal: maximize
  parameters: # parameter는 추가가능 합니다. 다만 추가할때마다 sweep.py의 변수를 바꿔주세요.
    batch_size:
      values: [8, 16]
    epochs:
      values: [1]
    lr:
      values : [0.00005,0.00002,0.00003,0.00001] #[0.00005,0.00003,0.00002,0.00001]
      # max: 0.0001
      # min: 0.00001

  